{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIcaJXjWcNnjWNUwFlHI/L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EvgeniaKantor/DI-Bootcamp_ML/blob/main/dchW7D2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Input Tensor (Word Embeddings):**\n",
        "\n",
        "Start with numerical representations of words (embeddings) because neural networks process numbers. This is the input data our self-attention mechanism will work on."
      ],
      "metadata": {
        "id": "OL4q_5TuCimc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sample input embeddings (6 tokens, each with 3-dimensional embedding)\n",
        "inputs = torch.tensor([\n",
        "    [0.43, 0.15, 0.89],  # your\n",
        "    [0.55, 0.87, 0.66],  # journey\n",
        "    [0.57, 0.85, 0.64],  # starts (our query)\n",
        "    [0.22, 0.58, 0.33],  # with\n",
        "    [0.77, 0.25, 0.10],  # one\n",
        "    [0.05, 0.80, 0.55]   # step\n",
        "])"
      ],
      "metadata": {
        "id": "0g2bpMjECnDM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Computing Attention Weights for Inputs[2]:**\n",
        "\n",
        "1.1.1 Attention Score:\n",
        "\n",
        "The dot product measures how similar two vectors are. Higher scores indicate greater similarity. We‚Äôre finding how relevant each word is to our ‚Äúquery‚Äù word."
      ],
      "metadata": {
        "id": "dxo9oet5C5wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select query vector (focus word)\n",
        "query = inputs[2]  # \"starts\"\n",
        "\n",
        "# 1.1.1 Attention Scores (dot product between query and all input vectors)\n",
        "attn_scores_2 = torch.zeros(inputs.size(0))\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n",
        "\n",
        "print(\"Attention Scores for 'starts':\", attn_scores_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vmGQJt6DnEK",
        "outputId": "917dd033-929a-4433-d02b-b0a76e2beb46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores for 'starts': tensor([0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first value (0.9422) corresponds to how related the word \"your\" is to \"starts\".\n",
        "\n",
        "The second value (1.4754) corresponds to the word \"journey\" and so on, down to \"step\"."
      ],
      "metadata": {
        "id": "gA1K8xd7D3cF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.2 Attention Weights:**\n",
        "\n",
        "Softmax transforms the scores into probabilities (attention weights). These weights represent how much ‚Äúattention‚Äù each word should receive when we create the context vector."
      ],
      "metadata": {
        "id": "EnkEMdGiD4vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# 1.1.2 Convert scores to Attention Weights using softmax\n",
        "attn_weights_2 = F.softmax(attn_scores_2, dim=0)\n",
        "print(\"Attention Weights:\", attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glFLoVAtEPP4",
        "outputId": "9d5f1be4-def3-4658-86e7-4dbc49a82ccd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights: tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "These values represent the normalized attention scores, where each score has been transformed into a probability that reflects how much each word should contribute to the context vector for the word \"starts\".\n",
        "\n",
        "The values sum to 1, as softmax ensures that we have a valid probability distribution.\n",
        "\n",
        "**Word-wise Breakdown:**\n",
        "\n",
        "The word \"journey\" (index 1) has the highest attention weight of 0.2369, meaning it's the most relevant word to \"starts\".\n",
        "\n",
        "The word \"starts\" itself (index 2) has an attention weight of 0.2326, indicating it‚Äôs very relevant to itself (which is expected).\n",
        "\n",
        "The word \"your\" (index 0) has the lowest attention weight of 0.1390, meaning it's less relevant to \"starts\" compared to other words."
      ],
      "metadata": {
        "id": "zzH0gRaKEryD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1.3 Context Vector:**\n",
        "\n",
        "The context vector is a weighted sum of the input vectors. It represents a refined version of the query, incorporating information from other relevant words."
      ],
      "metadata": {
        "id": "GLSOp5AZEyCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1.3 Compute Context Vector (weighted sum)\n",
        "context_vector_2 = torch.sum(attn_weights_2.unsqueeze(1) * inputs, dim=0)\n",
        "print(\"Context Vector for 'starts':\", context_vector_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DamSmk0_FBk2",
        "outputId": "4aa1a171-0323-47a4-a662-a4131acf9f72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector for 'starts': tensor([0.4431, 0.6496, 0.5671])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "This context vector is a weighted sum of all the word embeddings in the sequence, where the weights are given by the attention weights.\n",
        "\n",
        "Each value in this vector represents the \"refined\" version of the word \"starts\", considering the relationships (similarities) it has with the other words in the sequence.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "The context vector essentially \"represents\" the word \"starts\" in the context of the entire sentence, incorporating information about how each other word in the sentence is related to it.\n",
        "\n",
        "Since \"journey\" and \"starts\" are highly related (as seen from the attention weights), the context vector will reflect a strong influence from these words."
      ],
      "metadata": {
        "id": "BA2p3I2yFo6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Computing Attention Weights for All Inputs:**\n",
        "\n",
        "1.2.1 Attention Score:\n",
        "\n",
        "Extend the process to compute attention scores for every word against every other word in the sequence. This creates a matrix of relationships.\n"
      ],
      "metadata": {
        "id": "bcKKZzrEIiPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2.1 Compute attention scores matrix (dot product for each pair)\n",
        "attn_scores_all = torch.matmul(inputs, inputs.T)  # shape (6, 6)\n",
        "print(\"All Attention Scores:\\n\", attn_scores_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFyGrz9hJ2oN",
        "outputId": "cef2f9df-39d0-4059-bb03-8efa9ae02645"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Attention Scores:\n",
            " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a symmetric matrix where each element\n",
        "score\n",
        "ùëñ\n",
        "ùëó\n",
        "score\n",
        "ij\n",
        "‚Äã\n",
        "  represents the attention score between the word at position\n",
        "ùëñ\n",
        "i (row) and the word at position\n",
        "ùëó\n",
        "j (column).\n",
        "\n",
        "For example, the attention score between \"your\" (index 0) and \"starts\" (index 2) is 0.9422.\n",
        "\n",
        "The diagonal elements represent the self-attention scores, i.e., how much attention each word gives to itself. For instance, the self-attention score for \"starts\" (index 2) is 1.4570, meaning it's highly relevant to itself."
      ],
      "metadata": {
        "id": "Fek5aiS2LKs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.2 Attention Weights:**\n",
        "\n",
        "Apply softmax across rows to get attention weights for each word, showing its relationship to all others."
      ],
      "metadata": {
        "id": "90D3cmuiLeq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2.2 Apply softmax row-wise to get attention weights\n",
        "attn_weights_all = F.softmax(attn_scores_all, dim=1)\n",
        "print(\"All Attention Weights:\\n\", attn_weights_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeVeAmk_LO2l",
        "outputId": "bc711a21-9626-4a6a-af98-3be3a725b255"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Attention Weights:\n",
            " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These attention weights are the result of applying softmax to the attention scores. The values in this matrix represent the probability that a word (row) attends to another word (column).\n",
        "\n",
        "For example, for the word \"starts\" (index 2), it pays the most attention to \"journey\" (index 1) with an attention weight of 0.2369, which reflects how related these words are in the context."
      ],
      "metadata": {
        "id": "2v6JSD8dMEj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2.3 All Context Vector:**\n",
        "\n",
        "Generate a context vector for each word, capturing its meaning in the context of the entire sequence."
      ],
      "metadata": {
        "id": "tC9aKUuFMnIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2.3 Generate context vectors (weighted sum for each word)\n",
        "context_vectors_all = torch.matmul(attn_weights_all, inputs)\n",
        "print(\"All Context Vectors:\\n\", context_vectors_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye-U6A8kMxwT",
        "outputId": "0096b0c3-c727-4534-873b-23c9568f5fa0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Context Vectors:\n",
            " tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each context vector is a weighted sum of the input embeddings, where the weights are the attention weights calculated previously.\n",
        "\n",
        "For example, the context vector for \"starts\" (index 2) is tensor([0.4431, 0.6496, 0.5671]), which represents a refined version of the word \"starts\" in the context of the entire sequence."
      ],
      "metadata": {
        "id": "6G0OGVJMNJ_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. The ‚ÄòSelf‚Äô in Self-Attention**¬∂\n",
        "\n",
        "In self-attention, the ‚Äòself‚Äô refers to the mechanism‚Äôs ability to computer attention weights by relating different positions within a single input sequence.\n",
        "\n",
        "2.1 Weights Parameters vs Attention Weights\n",
        "\n",
        "In the weight matrices W, the term ‚Äòweight‚Äô is short for ‚Äòweight parameters‚Äô, the values of a neural network that are optimized during training. This is not to be confused with the attention weights.\n",
        "\n",
        "As I already saw in the previous section, attention weights determin the extent to which a context vector depends on the different parts of input, i.e., to what ectent the network focuses on different parts of the input.\n",
        "\n",
        "In summary, weight parameters are the fundamental, learned coefficents that definedthe networks connection, while attention weights are dynamic, context specific values.\n",
        "\n",
        "**2.1 Weights Parameters vs Attention Weights:**\n",
        "\n",
        "Distinguish between learned parameters (weights of the network) and dynamically computed attention weights. This clarifies the different roles they play."
      ],
      "metadata": {
        "id": "r-XGz3qKNJ8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weights Parameters (Wq, Wk, Wv):**\n",
        "\n",
        "Weights Parameters are the learnable parameters (i.e., the weights) in the neural network. These parameters are learned during the training process and define the connection between neurons in the network.\n",
        "\n",
        "In the case of self-attention, Wq (query weight), Wk (key weight), and Wv (value weight) are the weight matrices used to transform the input word embeddings into queries, keys, and values, respectively.\n",
        "\n",
        "**Attention Weights:**\n",
        "\n",
        "Attention Weights are computed dynamically during inference based on the input sequence. These weights indicate how much attention each word should pay to the other words in the sequence when constructing the context vector for each word.\n",
        "\n",
        "The attention weights are context-specific and depend on the similarity (via dot-product) between the query and key vectors."
      ],
      "metadata": {
        "id": "vAS6SmyYOkTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Computing Weight Parameters for Inputs[2]:**\n",
        "\n",
        "2.2.1 Initialize the three weight matrices Wq, Wk, Wv:\n",
        "Introduce learnable weight matrices (Wq, Wk, Wv) to transform input vectors into queries, keys, and values. This adds flexibility and allows the model to learn complex relationships."
      ],
      "metadata": {
        "id": "DDxtDy2nY_Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weight matrices (random for simplicity)\n",
        "Wq = torch.rand(3, 3)  # Query weight matrix (3x3)\n",
        "Wk = torch.rand(3, 3)  # Key weight matrix (3x3)\n",
        "Wv = torch.rand(3, 3)  # Value weight matrix (3x3)"
      ],
      "metadata": {
        "id": "jcFzaKBFZSUG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.2 Compute the query, key, and value vectors for inputs[1]:**\n",
        "\n",
        "These transformations project the input into different ‚Äúspaces‚Äù that emphasize different aspects of the word‚Äôs meaning."
      ],
      "metadata": {
        "id": "IP9jFMUDZp3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select input index 1 (\"journey\")\n",
        "input_1 = inputs[1]\n",
        "\n",
        "# Compute the query, key, and value vectors using the weight matrices\n",
        "query_1 = torch.matmul(input_1, Wq)\n",
        "key_1 = torch.matmul(input_1, Wk)\n",
        "value_1 = torch.matmul(input_1, Wv)\n",
        "\n",
        "print(f\"Query vector for 'journey': {query_1}\")\n",
        "print(f\"Key vector for 'journey': {key_1}\")\n",
        "print(f\"Value vector for 'journey': {value_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gxFFCWdaTCh",
        "outputId": "ec6dcf33-6386-4f31-900f-b849f2ccece4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vector for 'journey': tensor([1.0983, 1.7410, 1.5428])\n",
            "Key vector for 'journey': tensor([1.4116, 0.7436, 0.9073])\n",
            "Value vector for 'journey': tensor([1.2868, 1.5327, 0.7423])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.3 Compute the Attention Score inputs[1][1] or œâ11:**\n",
        "\n",
        "Calculate the similarity between the transformed query and key."
      ],
      "metadata": {
        "id": "4Qs2k7GkalBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the attention score between query_1 and key_1\n",
        "attn_score_1_1 = torch.dot(query_1, key_1)\n",
        "\n",
        "print(f\"Attention score (œâ11) between 'journey' query and key: {attn_score_1_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OInsloBrarq0",
        "outputId": "330a61fd-a74e-48dc-d5c0-e2ac651b806d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention score (œâ11) between 'journey' query and key: 4.244719982147217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.4 Compute all the Attention Scores for inputs[1]:**\n",
        "\n",
        "Calculate all the similarity scores against the query vector."
      ],
      "metadata": {
        "id": "mJ1Z2PlubUXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute all attention scores between query_1 and all keys (dot products)\n",
        "attn_scores_all_1 = torch.matmul(inputs, Wk.T)  # Dot product between all inputs and Wk\n",
        "print(\"All Attention Scores for 'journey':\\n\", attn_scores_all_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvPwU1UVbWaH",
        "outputId": "b4e71fbc-798f-40c2-fa8f-f63f713ba283"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Attention Scores for 'journey':\n",
            " tensor([[0.5232, 1.1521, 0.3574],\n",
            "        [0.7191, 1.3298, 0.7116],\n",
            "        [0.7241, 1.3226, 0.7098],\n",
            "        [0.3542, 0.6728, 0.3956],\n",
            "        [0.6108, 0.8200, 0.4761],\n",
            "        [0.3336, 0.7944, 0.4392]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.5 Attention weights for inputs[1]:**\n",
        "\n",
        "Normalize the attention scores."
      ],
      "metadata": {
        "id": "rfUpRurZbhux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply softmax to the attention scores to get attention weights\n",
        "attn_weights_1 = F.softmax(attn_scores_all_1, dim=0)\n",
        "print(\"Attention Weights for 'journey':\\n\", attn_weights_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7njBz9vNbmW7",
        "outputId": "d84b0224-2952-4ef4-ed6a-0bda16e6d528"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights for 'journey':\n",
            " tensor([[0.1612, 0.1846, 0.1409],\n",
            "        [0.1961, 0.2205, 0.2008],\n",
            "        [0.1971, 0.2189, 0.2004],\n",
            "        [0.1362, 0.1143, 0.1464],\n",
            "        [0.1760, 0.1325, 0.1586],\n",
            "        [0.1334, 0.1291, 0.1529]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2.6 Calculate Context vector for inputs[1]:**\n",
        "\n",
        "Generate the context vector."
      ],
      "metadata": {
        "id": "5lM4BHU5bzH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute context vector for 'journey' by taking weighted sum of input vectors\n",
        "context_vector_1 = torch.sum(attn_weights_1.unsqueeze(1) * inputs, dim=0)\n",
        "print(\"Context Vector for 'journey':\", context_vector_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ7v5EIEbt22",
        "outputId": "d7aaef09-01d0-450b-cfeb-4574bdb9d82d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector for 'journey': tensor([[0.4300, 0.1500, 0.8900],\n",
            "        [0.5500, 0.8700, 0.6600],\n",
            "        [0.5700, 0.8500, 0.6400],\n",
            "        [0.2200, 0.5800, 0.3300],\n",
            "        [0.7700, 0.2500, 0.1000],\n",
            "        [0.0500, 0.8000, 0.5500]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Computing Weight Parameters for All Inputs:**\n",
        "\n",
        "**2.3.2 Compute the query, key, and value vectors:**\n",
        "\n",
        "Compute the transformed vectors for all input words.\n"
      ],
      "metadata": {
        "id": "SpqqojW4cAD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the query, key, and value vectors for all inputs\n",
        "queries = torch.matmul(inputs, Wq)\n",
        "keys = torch.matmul(inputs, Wk)\n",
        "values = torch.matmul(inputs, Wv)\n",
        "\n",
        "print(\"All Queries:\\n\", queries)\n",
        "print(\"All Keys:\\n\", keys)\n",
        "print(\"All Values:\\n\", values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp2HEdWlcNXH",
        "outputId": "75233322-6c62-4ef3-d671-a6fd3bfabedc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Queries:\n",
            " tensor([[0.6376, 1.1715, 1.1689],\n",
            "        [1.0983, 1.7410, 1.5428],\n",
            "        [1.1020, 1.7286, 1.5322],\n",
            "        [0.5779, 0.9429, 0.8136],\n",
            "        [0.8592, 1.0181, 0.9099],\n",
            "        [0.5825, 1.1272, 0.9741]])\n",
            "All Keys:\n",
            " tensor([[0.8316, 0.5522, 0.3095],\n",
            "        [1.4116, 0.7436, 0.9073],\n",
            "        [1.3995, 0.7317, 0.8929],\n",
            "        [0.7891, 0.4133, 0.5618],\n",
            "        [0.7879, 0.3115, 0.3833],\n",
            "        [0.9565, 0.5554, 0.7301]])\n",
            "All Values:\n",
            " tensor([[0.8957, 1.0611, 0.4683],\n",
            "        [1.2868, 1.5327, 0.7423],\n",
            "        [1.2731, 1.5030, 0.7403],\n",
            "        [0.7049, 0.8791, 0.3975],\n",
            "        [0.6687, 0.5438, 0.4965],\n",
            "        [0.8842, 1.2215, 0.4439]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.3 Compute the Attention Score for all inputs:**\n",
        "\n",
        "Compute all attention scores between all words."
      ],
      "metadata": {
        "id": "khK0yxFncbty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute all attention scores (dot product between queries and keys)\n",
        "attn_scores_all = torch.matmul(queries, keys.T)\n",
        "print(\"All Attention Scores:\\n\", attn_scores_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpLE9Hegcu62",
        "outputId": "8d9f65dc-a221-4098-e632-ec53d9d1500d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Attention Scores:\n",
            " tensor([[1.5389, 2.8317, 2.7933, 1.6440, 1.3154, 2.1140],\n",
            "        [2.3522, 4.2447, 4.1886, 2.4529, 1.9990, 3.1440],\n",
            "        [2.3451, 4.2311, 4.1753, 2.4447, 1.9941, 3.1329],\n",
            "        [1.2531, 2.2551, 2.2252, 1.3028, 1.0609, 1.6705],\n",
            "        [1.5584, 2.7955, 2.7600, 1.6100, 1.3429, 2.0517],\n",
            "        [1.4083, 2.5443, 2.5099, 1.4728, 1.1835, 1.8945]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.4 Attention weights for all inputs:**\n",
        "\n",
        "Normalize the attention scores."
      ],
      "metadata": {
        "id": "cJwCW4gjc27V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the attention scores with softmax\n",
        "attn_weights_all = F.softmax(attn_scores_all, dim=1)\n",
        "print(\"All Attention Weights:\\n\", attn_weights_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9-wH7LPc8Sf",
        "outputId": "55f666de-a7cb-4f68-9fd0-4f95d4f68efe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Attention Weights:\n",
            " tensor([[0.0845, 0.3078, 0.2962, 0.0938, 0.0676, 0.1502],\n",
            "        [0.0558, 0.3702, 0.3500, 0.0617, 0.0392, 0.1231],\n",
            "        [0.0561, 0.3697, 0.3496, 0.0619, 0.0395, 0.1233],\n",
            "        [0.1024, 0.2790, 0.2708, 0.1077, 0.0845, 0.1555],\n",
            "        [0.0887, 0.3058, 0.2951, 0.0934, 0.0715, 0.1453],\n",
            "        [0.0942, 0.2934, 0.2835, 0.1005, 0.0752, 0.1532]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3.5 Calculate Context vector for all inputs:**\n",
        "\n",
        "Generate all context vectors."
      ],
      "metadata": {
        "id": "iZ-E9g2GdEha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute all context vectors by taking weighted sum of input vectors\n",
        "context_vectors_all = torch.matmul(attn_weights_all, values)\n",
        "print(\"All Context Vectors:\\n\", context_vectors_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiNjzKhldIbR",
        "outputId": "c5df5a03-45a5-4326-8e98-4dca67ba75de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Context Vectors:\n",
            " tensor([[1.0929, 1.3092, 0.6248],\n",
            "        [1.1505, 1.3786, 0.6587],\n",
            "        [1.1500, 1.3780, 0.6584],\n",
            "        [1.0655, 1.2740, 0.6094],\n",
            "        [1.0910, 1.3050, 0.6242],\n",
            "        [1.0794, 1.2921, 0.6171]])\n"
          ]
        }
      ]
    }
  ]
}